{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Для воспроизводимости\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN intro\n",
    "\n",
    "Давайте разберемся что из себя вообще представляют рекуррентные нейронные сети в самом простом виде.\n",
    "\n",
    "<img src=\"./pics/rnn.png\" width=\"90%\">\n",
    "\n",
    "В самом простом виде для одного входного вектора $x_{(t)}$ и одного слоя рекуррентной сети справедливо такое соотношение:\n",
    "\n",
    "$$y_{(t)} = \\phi (x_{(t)}^T \\cdot w_x + y_{(t-1)}^T \\cdot w_y + b)$$\n",
    "\n",
    "где \n",
    "* $x(t)$ -- входной вектор на текущем шаге\n",
    "* $y(t)$ -- выходной вектор на текущем шаге\n",
    "* $w_x$ -- вектор весов нейронов для входа\n",
    "* $w_y$ -- вектор весов нейронов для выхода\n",
    "* $y(t-1)$ -- выходной вектор с прошлого шага. Для шага 0 этот вектор нулевой\n",
    "* $b$ -- байес (bias)\n",
    "* $\\phi$ -- обозначение для функции активации, например ReLU\n",
    "\n",
    "\n",
    "Есть понятие **hidden_state** ( $h(t)$ ) -- это \"память\" рекуррентной ячейки.\n",
    "\n",
    "В общем случае $h_{(t)} = f(h_{(t-1)}, x_{(t)})$, но выход также $y{(t)} = f(h{(t-1)}, x{(t)})$.\n",
    "\n",
    "В данном случае $h(t) == y(t)$, но на практике используются более сложные архитектуры и в них **hidden_state** не совпадает с непосредственным выходом нейросетки.\n",
    "\n",
    "------\n",
    "\n",
    "## Напишем свою простую RNN сеть\n",
    "\n",
    "Снова немножко математики чтобы привести формулу выше к более удобному виду.\n",
    "\n",
    "Представим, что на вход подается не один вектор $x_{(t)}$, а целый мини-батч размера $m$ таких векторов $X_{(t)}$, соответственно все дальнейшие размышления мы уже производим в матричном виде:\n",
    "\n",
    "$$ Y_{(t)} = \\phi(X_{(t)}^T \\cdot W_x + Y_{(t-1)}^T \\cdot W_y + b) = \\phi([X_{(t)} Y_{(t-1)}] \\cdot W + b) $$\n",
    "где\n",
    "$$ W = [W_x W_y]^T $$\n",
    "\n",
    "*Операция в скобках квадратных -- конкатенация матриц\n",
    "\n",
    "По размерностям:\n",
    "* $Y_{(t)}$ -- матрица [$m$ x n_neurons]\n",
    "* $X_{(t)}$ -- матрица [$m$ x n_features]\n",
    "* $b$ -- вектор длины n_neurons\n",
    "* $W_x$ -- веса между входами и нейронами размерностью [n_features x n_neurons]\n",
    "* $W_y$ -- веса связей с прошлым выходом размерностью [n_neurons x n_neurons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "# Напишем нейронку прямо как на картинке в самом верху с 5-ю нейронами\n",
    "# На вход будем подавать векторы длины 3\n",
    "n_features = 3\n",
    "n_neurons = 5\n",
    "\n",
    "# С текушей имплементацией наша нейронка делает всего 2 шага\n",
    "X0 = tf.placeholder(tf.float32, [None, n_features])\n",
    "X1 = tf.placeholder(tf.float32, [None, n_features])\n",
    "\n",
    "Wx = tf.Variable(tf.random_normal(shape=[n_features, n_neurons], dtype=tf.float32))\n",
    "Wy = tf.Variable(tf.random_normal(shape=[n_neurons, n_neurons], dtype=tf.float32))\n",
    "b = tf.Variable(tf.zeros([1, n_neurons], dtype=tf.float32))\n",
    "\n",
    "# Здесь в качестве функции phi берем гиперболический тангенс\n",
    "Y0 = tf.tanh(tf.matmul(X0, Wx) + b)\n",
    "Y1 = tf.tanh(tf.matmul(Y0, Wy) + tf.matmul(X1, Wx) + b)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot use the default session to execute operation: the operation's graph is different from the session's graph. Pass an explicit session to run(session=sess).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-fd3abcab6dbd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0minit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mY0_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY1_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mY0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX0_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX1_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m   1704\u001b[0m         \u001b[0mnone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1705\u001b[0m     \"\"\"\n\u001b[1;32m-> 1706\u001b[1;33m     \u001b[0m_run_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[1;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   3952\u001b[0m                        \"`run(session=sess)`\")\n\u001b[0;32m   3953\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3954\u001b[1;33m       raise ValueError(\"Cannot use the default session to execute operation: \"\n\u001b[0m\u001b[0;32m   3955\u001b[0m                        \u001b[1;34m\"the operation's graph is different from the \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3956\u001b[0m                        \u001b[1;34m\"session's graph. Pass an explicit session to \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot use the default session to execute operation: the operation's graph is different from the session's graph. Pass an explicit session to run(session=sess)."
     ]
    }
   ],
   "source": [
    "# Будем подавать на вход мини батчи размером 4\n",
    "X0_batch = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 0, 1]])  # t = 0\n",
    "X1_batch = np.array([[9, 8, 7], [0, 0, 0], [6, 5, 4], [3, 2, 1]])  # t = 1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    Y0_val, Y1_val = sess.run([Y0, Y1], feed_dict={X0: X0_batch, X1: X1_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0664006 ,  0.96257669,  0.68105793,  0.70918542, -0.89821601],\n",
       "       [ 0.9977755 , -0.71978903, -0.99657607,  0.96739239, -0.99989718],\n",
       "       [ 0.99999774, -0.99898803, -0.99999893,  0.99677622, -0.99999988],\n",
       "       [ 1.        , -1.        , -1.        , -0.99818915,  0.99950868]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y0_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -1.        , -1.        ,  0.40200251, -0.99999982],\n",
       "       [-0.12210421,  0.62805271,  0.96718431, -0.99371219, -0.25839362],\n",
       "       [ 0.99999827, -0.9999994 , -0.9999975 , -0.85943311, -0.99998808],\n",
       "       [ 0.99928284, -0.99999815, -0.99990582,  0.98579627, -0.92205757]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y1_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание\n",
    "\n",
    "Напишите то же самое, но использовав всего одно матричное перемножение на каждом шаге (см формулу в объяснении выше)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X0 = tf.placeholder(tf.float32, [None, n_features])\n",
    "X1 = tf.placeholder(tf.float32, [None, n_features])\n",
    "\n",
    "# здесь код объявления новых переменных, а также построения графа вычислений \n",
    "W = tf.Variable(tf.random_normal(shape=[n_features + n_neurons, n_neurons], dtype=tf.float32))\n",
    "b = tf.Variable(tf.zeros([1, n_neurons]), dtype=tf.float32)\n",
    "Y0 = tf.tanh(tf.matmul(X0, W[:n_features,:]) + b)\n",
    "Y1 = tf.tanh(tf.matmul(tf.concat([X1,Y0],1), W) + b)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    Y0_val_1, Y1_val_1 = sess.run([Y0, Y1], feed_dict={X0: X0_batch, X1: X1_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0664006 ,  0.96257669,  0.68105793,  0.70918542, -0.89821601],\n",
       "       [ 0.9977755 , -0.71978903, -0.99657607,  0.96739239, -0.99989718],\n",
       "       [ 0.99999774, -0.99898803, -0.99999893,  0.99677622, -0.99999988],\n",
       "       [ 1.        , -1.        , -1.        , -0.99818915,  0.99950868]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y0_val_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -1.        , -1.        ,  0.99938881, -1.        ],\n",
       "       [ 0.93467975,  0.98571068,  0.999246  , -0.63660294,  0.97086155],\n",
       "       [ 0.99999988, -0.99983609, -0.99989033,  0.53755277, -0.99450821],\n",
       "       [ 0.99932444, -0.83284515, -0.99992496, -0.78478205,  0.95726931]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y1_val_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic_rnn\n",
    "\n",
    "В tf есть функция `tf.contrib.rnn.static_rnn` которая создает для каждого unrolling'а (т.е. каждого батча на входе) отдельную ячейку того типа, который мы ей передадим. В данном случае наша имплементация совпадает с имплементацией `tf.contrib.rnn.BasicRNNCell` в tf. Это все не очень круто, если требуется сделать большое число шагов -- у нас попросту может закончится память, если мы вдруг решим делать backprop. Поэтому к счастью есть другой путь -- это `dynamic_rnn`.\n",
    "\n",
    "Сделаем весь предыдущий пример с помощью `dynamic_rnn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_steps = 2\n",
    "n_features = 3\n",
    "n_neurons = 5\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_features])\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_length = tf.placeholder(tf.int32, [None])\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32,\n",
    "                                    sequence_length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "X_batch = np.array([\n",
    "        # step 0     step 1\n",
    "        [[0, 1, 2], [9, 8, 7]], # instance 1\n",
    "        [[3, 4, 5], [0, 0, 0]], # instance 2 (padded with zero vectors)\n",
    "        [[6, 7, 8], [6, 5, 4]], # instance 3\n",
    "        [[9, 0, 1], [3, 2, 1]], # instance 4\n",
    "    ])\n",
    "\n",
    "# параметр с истинной длиной последовательностей\n",
    "seq_length_batch = np.array([2, 1, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    outputs_val, states_val = sess.run(\n",
    "        [outputs, states], feed_dict={X: X_batch, seq_length: seq_length_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2, 5)\n",
      "(4, 5)\n"
     ]
    }
   ],
   "source": [
    "print(outputs_val.shape)\n",
    "print(states_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.68579948 -0.25901747 -0.80249101 -0.18141513 -0.37491536]\n",
      "  [-0.99996698 -0.94501185  0.98072106 -0.9689762   0.99966913]]\n",
      "\n",
      " [[-0.99099374 -0.64768541 -0.67801034 -0.7415446   0.7719509 ]\n",
      "  [ 0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[-0.99978048 -0.85583007 -0.49696958 -0.93838578  0.98505187]\n",
      "  [-0.99951065 -0.89148796  0.94170523 -0.38407657  0.97499216]]\n",
      "\n",
      " [[-0.02052618 -0.94588047  0.99935204  0.37283331  0.9998163 ]\n",
      "  [-0.91052347  0.05769409  0.47446665 -0.44611037  0.89394671]]]\n"
     ]
    }
   ],
   "source": [
    "# заметим, что у второго инпута есть нули в final state\n",
    "print(outputs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.23427293 -0.98320025  1.          0.05531176  0.13038807]\n",
      " [ 0.06481666 -0.98885179  0.99938971 -0.2837334   0.94707835]\n",
      " [-0.3956297  -0.49045283  0.99999118  0.26839557 -0.66423571]\n",
      " [ 0.65352577 -0.56341326  0.99020594  0.72314984  0.30071661]]\n"
     ]
    }
   ],
   "source": [
    "# а тут уже нет нулей;\n",
    "# так происходит благодаря наличию параметра seq_length\n",
    "print(states_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генерация имен\n",
    "\n",
    "А теперь попробуем понять что можно со всем этим вышеперечисленным делать полезного.\n",
    "\n",
    "_Teaser:_\n",
    "\n",
    "* Сложно придумать имя для переменной? Но куда сложнее придумать хорошее имя для человека.\n",
    "  Поэтому давайте напишем нейронку, которая сделает это за нас.\n",
    "* Набор данных содержит ~ 8 тыс человеческих имен из разных культур [в латинской расшифровке]\n",
    "* Цель (игрушечная проблема): изучить генеративную модель по именам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "start_token = \" \"\n",
    "\n",
    "with open(\"names\") as f:\n",
    "    names = f.read()[:-1].split('\\n')\n",
    "    names = [start_token + name.lower() for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n samples =  7944\n",
      "Abagael\n",
      "Claresta\n",
      "Glory\n",
      "Liliane\n",
      "Prissie\n",
      "Geeta\n",
      "Giovanne\n",
      "Piggy\n"
     ]
    }
   ],
   "source": [
    "print('n samples = ', len(names))\n",
    "for x in names[::1000]:\n",
    "    print(x.strip().capitalize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обработка текста\n",
    "\n",
    "Рассмотрим все буквы без учета регистра + ')' -- конец имени"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens =  30\n"
     ]
    }
   ],
   "source": [
    "# Найдем все уникальные символы\n",
    "# Без учета регистра\n",
    "\n",
    "token_set = set()\n",
    "for name in names:\n",
    "    for letter in name:\n",
    "        token_set.add(letter)\n",
    "\n",
    "\n",
    "token_set.add(')')\n",
    "tokens = list(token_set)\n",
    "tokens.sort()\n",
    "\n",
    "print('n_tokens = ', len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!token_to_id = < словарь символов -> их id (index в tokens list)>\n",
    "token_to_id = {t: i for i, t in enumerate(tokens)}\n",
    "\n",
    "#!id_to_token = < словарь айдишников -> соответствующие символы >\n",
    "id_to_token = {i: t for i, t in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построим распределение длин всех имен"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f4870afac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(list(map(len, names)))\n",
    "\n",
    "# Посмотрим какая максимальная длина у имени в этом датасете\n",
    "MAX_LEN = min([60, max(list(map(len, names)))])-1 \n",
    "print(MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Переведем все символы в их id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names_ix = list(map(lambda name: list(map(token_to_id.get, name + ')')), names))\n",
    "\n",
    "\n",
    "# Добьем нулями короткие имена до MAX_LEN и усечем слишком длинные\n",
    "for i in range(len(names_ix)):\n",
    "    names_ix[i] = names_ix[i][:MAX_LEN+1] #crop too long\n",
    "    \n",
    "    if len(names_ix[i]) < MAX_LEN+1:\n",
    "        names_ix[i] += [token_to_id[\" \"]]*(MAX_LEN+1 - len(names_ix[i])) #pad too short\n",
    "        \n",
    "assert len(set(map(len, names_ix))) == 1\n",
    "\n",
    "names_ix = np.array(names_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  4,  5,  4, 10,  4,  8, 15,  2,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  4,  5,  4, 10,  4, 12, 15,  2,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  4,  5,  5,  8,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  4,  5,  5,  8, 28,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  4,  5,  5, 12,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  4,  5,  5, 12,  8,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  4,  5,  5, 28,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  4,  5, 12, 10,  4,  8, 15,  2,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  4,  5, 12, 10,  4, 12, 15,  2,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  4,  5, 12, 10,  4, 15,  8,  2,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_ix[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерилка батчей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_batch(data, batch_size):\n",
    "    \n",
    "    rows = data[np.random.randint(0, len(data), size=batch_size)]\n",
    "    x = rows[:, :-1]\n",
    "    y = rows[:, 1:]\n",
    "    \n",
    "    count = lambda r: np.sum([id_to_token[t] != ' ' for t in r])\n",
    "    lengths = list(map(count, x))\n",
    "    \n",
    "    return x, y, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 15)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y, length = sample_batch(names_ix, 10)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 12, 21, 26, 12, 17,  2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  5,  8, 21, 17,  4,  7, 12, 17,  4,  2,  0,  0,  0,  0],\n",
       "       [ 0, 16,  4, 10,  7,  4, 15,  8, 17,  4,  2,  0,  0,  0,  0],\n",
       "       [ 0, 12, 22,  8,  4,  5,  4, 15,  2,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0, 14, 12, 21,  4,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0, 15,  8, 18, 17, 12,  7,  2,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0, 25, 12, 25,  4,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  8, 15, 15, 12, 22,  2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0, 13, 18, 22,  8,  9, 12, 17,  4,  2,  0,  0,  0,  0,  0],\n",
       "       [ 0, 11, 18, 15, 15,  8, 28,  2,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12, 21, 26, 12, 17,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 5,  8, 21, 17,  4,  7, 12, 17,  4,  2,  0,  0,  0,  0,  0],\n",
       "       [16,  4, 10,  7,  4, 15,  8, 17,  4,  2,  0,  0,  0,  0,  0],\n",
       "       [12, 22,  8,  4,  5,  4, 15,  2,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [14, 12, 21,  4,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [15,  8, 18, 17, 12,  7,  2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [25, 12, 25,  4,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 8, 15, 15, 12, 22,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [13, 18, 22,  8,  9, 12, 17,  4,  2,  0,  0,  0,  0,  0,  0],\n",
       "       [11, 18, 15, 15,  8, 28,  2,  0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 9, 7, 6, 6, 8, 4, 10, 7]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Входы сетки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "# подразумевается, что размерность X [batch_size, max_length];\n",
    "X = tf.placeholder(tf.int32, [None, None], name= 'X')\n",
    "y = tf.placeholder(tf.int32, [None, None], name = 'y')\n",
    "lengths = tf.placeholder(tf.int32, [None], name = 'lengths')\n",
    "learning_rate_ph = tf.placeholder(dtype=tf.float32, shape=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_neurons = 60\n",
    "embedding_size = 8\n",
    "vocabulary_size = len(tokens)\n",
    "\n",
    "n_steps = MAX_LEN # Этот параметр совпадает\n",
    "                  # с максимальной длиной последовательности, \n",
    "                  # которая может быть подана на вход\n",
    "                  # иначе говоря, это unrollings\n",
    "\n",
    "# для входной последовательности создаем матрицу эмбеддингов\n",
    "embedding_mtx = tf.get_variable(name='embeddings', shape=[vocabulary_size, embedding_size])\n",
    "\n",
    "# достаем из матрицы эмбеддингов нужные нам векторы X\n",
    "embed = tf.nn.embedding_lookup(embedding_mtx, X)\n",
    "\n",
    "\n",
    "cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.tanh)\n",
    "rnn_outputs, states = tf.nn.dynamic_rnn(cell=cell, inputs=embed,\n",
    "                                        sequence_length=lengths, dtype=tf.float32)\n",
    "\n",
    "# получаем ненормированное распределение по классам \n",
    "# для каждого анроллинга в каждом сэмле в батче\n",
    "pred_logits = tf.layers.dense(inputs=rnn_outputs, units=vocabulary_size, name='output_projection')\n",
    "\n",
    "# кодируем one-hot классы, т.к. это тоже нужно функции лосса\n",
    "labels_one_hot = tf.one_hot(y, depth=vocabulary_size, dtype=tf.float32)\n",
    "\n",
    "# считаем илололосс\n",
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=labels_one_hot,\n",
    "    logits=pred_logits)\n",
    "    \n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "\n",
    "pred_probas = tf.nn.softmax(pred_logits)\n",
    "\n",
    "# берем максимум по оси, соответствующей количеству классов\n",
    "# получаем матрицу размера [batch_size, num_steps]\n",
    "prediction = tf.argmax(pred_probas, axis=2)\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(learning_rate_ph).minimize(loss)\n",
    "\n",
    "# берем распределение вероятностей только для последнего символа в каждом сэмпле\n",
    "# это потребуется для генерации\n",
    "last_word_probas = pred_probas[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как можно узнать о параметрах, которые тренируются"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'embeddings:0' shape=(30, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/basic_rnn_cell/weights:0' shape=(68, 60) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/basic_rnn_cell/biases:0' shape=(60,) dtype=float32_ref>,\n",
       " <tf.Variable 'output_projection/kernel:0' shape=(60, 30) dtype=float32_ref>,\n",
       " <tf.Variable 'output_projection/bias:0' shape=(30,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Напишем функцию, позволяющую генерить имена по затравке\n",
    "\n",
    "*Что делается*\n",
    "\n",
    "* Берется затравка (seed_phrase)\n",
    "* Предсказывается вероятность появления следующего токена\n",
    "* Следующий токен сэмплируется из распределения, предсказанного моделью\n",
    "* Полученный токен добавляется к затравке\n",
    "* Повторяем с шага 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_sample(sess, seed_phrase=None, N=MAX_LEN, n_snippets=1):\n",
    "    \n",
    "    if seed_phrase is None:\n",
    "        seed_phrase = ' '\n",
    "    elif seed_phrase[0].isalpha():\n",
    "        seed_phrase = ' ' + seed_phrase\n",
    "    seed_phrase = seed_phrase.lower()\n",
    "    seed_phrase = np.array([token_to_id[tok] for tok in seed_phrase])\n",
    "    L = len(seed_phrase)\n",
    "    snippets = []\n",
    "    for _ in range(n_snippets):\n",
    "        x = np.zeros(N)\n",
    "        x[:len(seed_phrase)] = seed_phrase\n",
    "        for n in range(N - L):\n",
    "            feed_dict = {X: x[:L + n].reshape([1, -1]), lengths: [len(x)]}\n",
    "            p = sess.run(last_word_probas, feed_dict=feed_dict).reshape(-1)\n",
    "            ix = np.random.choice(np.arange(len(tokens)), p=p)\n",
    "            x[L + n] = ix\n",
    "        snippet = ''.join([id_to_token[idx] for idx in x])\n",
    "        if ')' in snippet:\n",
    "            upto = snippet.index(')')\n",
    "            snippet = snippet[:upto]\n",
    "        snippets.append(snippet.strip().capitalize())\n",
    "    return snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_pred(y_pred, k = 3):\n",
    "    \"\"\"\n",
    "    k: сколько вывести \n",
    "    предсказаний модели среди всех y_pred\n",
    "    \n",
    "    \"\"\"\n",
    "    for i in range(k):\n",
    "        print(\"\".join( [id_to_token[t] for t in y_pred[i,:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>Generated:  [\"Iz'm\", 'Gmx uz', 'Szelmhazvjoblr', 'Lzogsaggjtltof', 'Uddle pkjynvan', 'Wnrg']\n",
      "-------\n",
      "\n",
      "EPOCH:  0\n",
      "AVERAGE LOSS:  1.62943659818\n",
      ">>Predicted: \n",
      "aesni)         \n",
      "anrna)))       \n",
      "aerlnae)       \n",
      ">>Generated:  ['Erdisa', 'Eych', 'Eshie', 'Angostia', 'Astelie', 'Mesamet']\n",
      "-------\n",
      "\n",
      "EPOCH:  1\n",
      "AVERAGE LOSS:  1.09818096185\n",
      ">>Predicted: \n",
      "conii))e       \n",
      "chardh))))     \n",
      "cosi           \n",
      ">>Generated:  ['Welisa', 'Jolia', 'Meto', 'Addor', 'Gresarde', 'Gwedy']\n",
      "-------\n",
      "\n",
      "EPOCH:  2\n",
      "AVERAGE LOSS:  1.05015222597\n",
      ">>Predicted: \n",
      "cart)i))       \n",
      "cld)a          \n",
      "caren          \n",
      ">>Generated:  ['Marte', 'Costsa', 'Chento', 'Guaste', 'Patulee', 'Marista']\n",
      "-------\n",
      "\n",
      "EPOCH:  3\n",
      "AVERAGE LOSS:  1.03293791163\n",
      ">>Predicted: \n",
      "arristaaa)     \n",
      "aala           \n",
      "aanii)         \n",
      ">>Generated:  ['Karrobelneot', 'Kalasina', 'Marche', 'Amoosta', 'Dieturah', 'Sisuinae']\n",
      "-------\n",
      "\n",
      "EPOCH:  4\n",
      "AVERAGE LOSS:  1.01994612098\n",
      ">>Predicted: \n",
      "meenn          \n",
      "mariin)        \n",
      "maldi)         \n",
      ">>Generated:  ['Ckri-', 'Marcletta', 'Ally', 'Jossen', 'Ydamera', 'Davie']\n"
     ]
    }
   ],
   "source": [
    "s = tf.Session()\n",
    "    \n",
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "n_epochs = 5\n",
    "batches_per_epoch = 500\n",
    "batch_size = 10\n",
    "lr = 1e-2\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    print(\">>Generated: \", generate_sample(s, n_snippets=6))\n",
    "    print(\"-------\\n\")\n",
    "    avg_cost = 0\n",
    "    for batch in range(batches_per_epoch):\n",
    "        x_, y_, len_ = sample_batch(names_ix, batch_size)\n",
    "\n",
    "        _, iloss, y_pred = s.run([train_op, loss, prediction], {X: x_,\n",
    "                                                                y: y_,\n",
    "                                                                lengths: len_,\n",
    "                                                                learning_rate_ph: lr})\n",
    "        avg_cost += iloss\n",
    "\n",
    "    print(\"EPOCH: \", epoch)\n",
    "    print(\"AVERAGE LOSS: \", avg_cost/batches_per_epoch)\n",
    "    print(\">>Predicted: \")\n",
    "    print_pred(y_pred)\n",
    "\n",
    "print(\">>Generated: \", generate_sample(s, n_snippets=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Putileen',\n",
       " 'Putisterne',\n",
       " 'Putian',\n",
       " 'Putiley',\n",
       " 'Puticat',\n",
       " 'Putine',\n",
       " 'Putilleyntt',\n",
       " 'Putint',\n",
       " 'Putil',\n",
       " 'Putiane',\n",
       " 'Putiley',\n",
       " 'Putilea']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sample(s, seed_phrase='Puti', n_snippets=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Quilyn', 'Quricanne', 'Quertholina', 'Queby', 'Quera', 'Quudnia']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sample(s, seed_phrase='Q', n_snippets=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Eugwellin', 'Eugry', 'Eugwon', 'Eugtia', 'Eugeldond', 'Eugwey']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sample(s, seed_phrase='Eug', n_snippets=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Luondem',\n",
       " 'Lustileh',\n",
       " 'Luzedie',\n",
       " 'Luendaina',\n",
       " 'Luscio',\n",
       " 'Luzta',\n",
       " 'Luaria',\n",
       " 'Luggy',\n",
       " 'Lury',\n",
       " 'Lueltpa',\n",
       " 'Luann',\n",
       " 'Ludill',\n",
       " 'Luuston',\n",
       " 'Luene',\n",
       " 'Lublynn',\n",
       " 'Luskelyna',\n",
       " 'Luelte',\n",
       " 'Lustadinsh',\n",
       " 'Lurislie']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sample(s, seed_phrase='Lu', n_snippets=19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Попробуйте использовать несколько слоев рекуррентных сетей\n",
    "\n",
    "* Попробуйте поменять код модели, встроив туда модуль как в примере в следующей ячейке;\n",
    "* Попробуйте использовать другие cells: LSTM, GRU;\n",
    "* Попробуй генерировать твиты, скачав [датасет](http://study.mokoron.com) или какой угодно другой датасет\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Как ненапряжно замутить глубокую рекуррентную нейросеть\n",
    "\n",
    "n_neurons = 100\n",
    "n_layers = 3\n",
    "\n",
    "layers = [tf.contrib.rnn.BasicRNNCell(num_units=n_neurons,\n",
    "                                      activation=tf.nn.relu) for layer in range(n_layers)]\n",
    "multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)\n",
    "outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
